------------------------------------------------------------------------------
\\
arXiv:1810.04805
From: Ming-Wei Chang <xyz@example.com>
Date: Thu, 11 Oct 2018 00:50:01 GMT   (227kb,D)
Date (revised v2): Fri, 24 May 2019 20:37:26 GMT   (309kb,D)

Title: BERT: Pre-training of Deep Bidirectional Transformers for Language
  Understanding
Authors: Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova
Categories: cs.CL
License: http://arxiv.org/licenses/nonexclusive-distrib/1.0/
\\
  We introduce a new language representation model called BERT, which stands
for Bidirectional Encoder Representations from Transformers. Unlike recent
language representation models, BERT is designed to pre-train deep
bidirectional representations from unlabeled text by jointly conditioning on
both left and right context in all layers. As a result, the pre-trained BERT
model can be fine-tuned with just one additional output layer to create
state-of-the-art models for a wide range of tasks, such as question answering
and language inference, without substantial task-specific architecture
modifications.
  BERT is conceptually simple and empirically powerful. It obtains new
state-of-the-art results on eleven natural language processing tasks, including
pushing the GLUE score to 80.5% (7.7% point absolute improvement), MultiNLI
accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering
Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1
(5.1 point absolute improvement).
\\
